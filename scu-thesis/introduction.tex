\chapter{Introduction}

\section{Background and Motivation}
Data loss is a fundamental, widespread problem in computing. In a single year, 25\% of PC users will lose their data. Furthermore, 100\% of all data storage technologies, ranging from magnetic tapes to hard drives to solid state storage, will inevitably fail. \cite{imagineiti}  Therefore, the rule to apply is that if data only exists in one place, it doesn't exist at all.  Data loss is not only unavoidable, it is potentially devastating. 70\% of small businesses that experience major data loss go out of business within a year. \cite{imagineiti} Moreover, the U.S. loses an estimated \$18.2 Billion every year due to data loss. \cite{pepperdine} Almost every industry relies on accurately storing and accessing information, whether this be in the form of financial archives, software developmental codes, customer data, order records, etc. Backups, which are systems for duplicating oneâ€™s data and storing it in another place, counteract data loss.

Many current solutions implement data backup. We observe, however, that all systems suffer problems with privacy, accessibility, resilience, or a combination of all three. Three general categories describe current solutions: enterprise backup, cloud backup and personal backup. Unfortunately, current enterprise solutions cannot be examined as they are often highly customized and proprietary. Cloud backup is a service in which users upload files to be stored in a data center. These systems, while they are highly redundant, are not immune to corporate problems. If the company providing your backups goes out of business, your backups will no longer be accessible. In addition, these services may go down from time to time. Amazon S3, a common back end for such systems, has been known to have long outages. \cite{gigaom} \cite{rightscale}  And services such as Dropbox have suffered many security problems, such as when their password authentication simply failed on June 19th, 2011. \cite{dropbox_fail} Finally, there is the problem of privacy: when one uses a cloud backup service, one gives ownership of one's data to the corporation running the service.  Even though anyone who uses a cloud backup service should encrypt their data to preserve privacy, data encryption really just boils down to password protection, which is not reliable. \cite{password}  Also, most users would view manual encryption of their data as an inconvenient additional step to avoid.  Even if one trusts the individual corporation who runs the backup, it gives a single point of attack for hackers, criminals, or governments to steal one's data.  The 2014 celebrity photo hack is an example of this, in which vulnerabilities of Apple iCloud were exploited. \cite{theguardian}  Personal backup, on the other hand, is the solution used by diligent individuals who regularly save their files to an external storage device or media. While personal backup addresses the privacy and accessibility issues, the system is not resilient, since the external storage device will lose all the backup data if it ever fails. \cite{backblaze} In addition, accessibility to the data relies on constant access to a single device.


\section{Solution and Rationale}

\begin{wrapfigure}{r}{0.35\textwidth}
	\begin{center}
		\includegraphics[width=0.3\textwidth]{images/network.png}
	\end{center}
\end{wrapfigure}

To solve the problems presented by currently existing backup systems, our team proposes a peer-to-peer distributed backup system.  This system will be composed of a large number of users who are geographically distributed, ideally around the world.  Each user will bring the data that they wish to backup, and then split it up amongst several of the other users in a way that is geographically distributed and redundant.  This backup system will address the issues of privacy, accessibility, and resilience.  Because of the geographic distribution and the redundancy, the system will be resilient against the loss or destruction of nodes.  This is because even if one or multiple nodes goes down, the same data they stored will be replicated across many other nodes.
We imagine at least one storage system per network. These storage systems will be interconnected, allowing them to share data. The data will be distributed as widely as the networks using these devices, encouraging hardware redundancy and making sure the data is constantly accessible. The protocol, once designed, will be frozen, ensuring that these systems will continue to provide their services, even if support is no longer possible. In addition, our system greatly enhances privacy, since it contains no central entity or authority that is in charge of storing all the data.  Therefore, there is no single point of attack for hackers to go after to steal data, or an organization that a government can go after in order to force data turnover.  We ourselves as the designers of the system will not have access to the users' data, and each user's data will be only be accessible by the user who owns the data through encryption as mentioned above.

Our design centers around a storage device that provides an interface between a Local Area Network and the broader internet of storage devices. Within the network itself, a client-server architechure is adopted. The storage device acts as a smart server that manages the resources of clients with few resources. This is intended to reduce the requirements for the backup clients. This also centralizes the control of the backup system for the entire LAN. The storage device also interfaces with larger network of storage devices on other LANs. We choose a peer-to-peer architechure for this network to maximise the potential redundancy of our data placement algorithm.

\section{Related Work}
In the process of researching for our project, we tried to find similar systems that other groups had implemeneted.  One such system is one developed as a prototype by HP which follows a similar data sharing architecture we want to implement. \cite{hp} Like our system, this group's prototype used redundancy and geographic distribution to enhance reliability and accessibility.  Furthermore, their paper provided us with lots of information for the potential security gaps and exploits caused by such a system, and how to prevent them.  However, there are many shortcomings of their system that we hope ours will address.  First of all is the fact that their system never left the prototyping phase, so their user interface was not the best.  Secondly, their system did not do automatic encryption of data sent, which is something we want to add.  Finally, their system was not pure peer-to-peer but instead used a central server, which is a potential security weakness and failure point.

Other current products that use peer-to-peer data distribution are SyncThing and Crashplan. \cite{syncthing} \cite{crashplan} Similar to our system, both of these products were created to address the problem of privacy in cloud storage systems in which data is given to a third-party.  Both allow you to specify a list of computers which you trust, and automatically backup your data to them.  However, neither provides the automatic peer-to-peer distribution algorithm we wish to implement.  Therefore, the effort is left to the users to identify peers they can trust or even provide all the storage space themselves.

To aid the development of our own distribution algorithm, we consulted a paper by a research group at UC Berkeley.  \cite{scalable}  This group researched the application of hash tables to large distributed systems in order to perform functions such as load balancing, equal sharing of data, and seamless entries and departures of nodes into the network.  In this paper, they demonstrate how their Content-Addressable Network is ''scalabe, fault-tolerant and completely self-organizing.''  They did not actually develop a system for backing up and storing data as we did.  However this paper and the techniques it describes played a very large role in the development of our own system.  In the Architecture section of our paper, we describe in detail the application of hash tables to our system.

Finally, our team had to look into the various encryption and splitting technologies available which we could make use of in our system.  For example, we considered using ''secret splitting'', a technique in which backed up data is split into many seperate blocks for storage.  Individually, each block is useless and cannot be used to recover any data.  Information can only be recovered after a certain number of blocks are restored.  This technique would improve privacy and security in our system, since it would require attackers to steal many different data chunks from many different locations.  Another alternative is ''fountain code''.  Functionally, the ''fountain code'' technique is very similar to ''secret splitting''.  However, it was originally developed for the purpose of error correction in transmitted packets, and allowed for reconstruction of the entire message even with lost packets.  The use of this technique would allow data to be recovered from our network by only restoring a small number of data chunks.  This would be good for efficiency which in turn would aid reliability and accessibility.  However, it may be worse for security and privacy.